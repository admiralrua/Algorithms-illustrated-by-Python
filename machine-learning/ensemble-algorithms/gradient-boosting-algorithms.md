# Gradient boosting

These are boosting algorithms used when massive loads of data have to be handled in order to make predictions with high accuracy. Boosting is an ensemble learning algorithm that combines the predictive power of several base estimators to improve robustness. In short, it combines multiple weak or average predictors to build a strong predictor. These boosting algorithms always work well in data science competitions like Kaggle, AV Hackathon, CrowdAnalytix.

There are \(at least\) four algorithms belong to this family:

* GBM
* xGBoost
* LightGBM
* CatBoost



